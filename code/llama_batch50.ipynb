{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치마다 저장되도록 수정/ 생성된 데이터 추후 가공해야되지만 일단 Go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질의 생성 -> 가공 -> 질의-문서로 pos, neg 레이블 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 무엇->뭐\n",
    "- 시작할 때 나오는 숫자. 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953d16581daa4498b771f5cbd33ba8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # float16 사용으로 메모리 절약\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128256, 4096)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Special tokens 추가 및 임베딩 업데이트\n",
    "special_tokens = [\"<|eot_id|>\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 데이터 로드\n",
    "data_path = '../data/documents_copy.jsonl'\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"파일 {data_path}이(가) 존재하지 않습니다. 경로를 확인해 주세요.\")\n",
    "\n",
    "data = []\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 처리된 데이터 확인\n",
    "output_path = '../data/triples_data.jsonl'\n",
    "processed_doc_ids = set()\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, 'r', encoding='utf-8') as output_file:\n",
    "        for line in output_file:\n",
    "            entry = json.loads(line)\n",
    "            processed_doc_ids.add(entry['docid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 생성 함수\n",
    "def generate_questions(content, model, tokenizer, device, num_questions=3):\n",
    "    prompt = f\"다음 내용을 바탕으로 이해를 돕기 위한 {num_questions}개의 질문을 반말로 생성해줘:\\n내용: {content[:500]}\\n질문:\"  # 입력 길이 제한\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    eos_token_id = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():  # 메모리 사용 줄이기\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            eos_token_id=eos_token_id,\n",
    "            max_length=512,  # max_length 감소\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    questions = generated_text.split('질문:')[-1].strip().split('\\n')[:num_questions]\n",
    "    formatted_questions = {f'question{i+1}': question.strip() for i, question in enumerate(questions)}\n",
    "    return formatted_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2개의 새 데이터가 ../data/triples_data.jsonl에 추가 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4개의 새 데이터가 ../data/triples_data.jsonl에 추가 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6개의 새 데이터가 ../data/triples_data.jsonl에 추가 저장되었습니다.\n",
      "모든 새로 생성된 트리플 데이터가 ../data/triples_data.jsonl에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 트리플 데이터 준비 및 저장\n",
    "batch_size = 2\n",
    "new_triples_data = []\n",
    "\n",
    "for idx, entry in enumerate(data):\n",
    "    doc_id = entry['docid']\n",
    "\n",
    "    # 이미 처리된 docid는 건너뛰기\n",
    "    if doc_id in processed_doc_ids:\n",
    "        continue\n",
    "\n",
    "    content = entry['content']\n",
    "    questions = generate_questions(content, model, tokenizer, device)\n",
    "    new_triples_data.append({\n",
    "        'docid': doc_id,\n",
    "        'content': content,\n",
    "        'questions': questions,\n",
    "    })\n",
    "    torch.cuda.empty_cache()  # 캐시 정리로 메모리 확보\n",
    "\n",
    "    # 100개마다 새로 생성된 데이터만 저장\n",
    "    if (len(new_triples_data)) % batch_size == 0 or (idx + 1) == len(data):\n",
    "        with open(output_path, 'a', encoding='utf-8') as output_file:  # 'a' 모드로 추가 저장\n",
    "            for entry in new_triples_data:\n",
    "                json.dump(entry, output_file, ensure_ascii=False)\n",
    "                output_file.write('\\n')\n",
    "        new_triples_data = []  # 저장 후 리스트 초기화\n",
    "        print(f\"{idx + 1}개의 새 데이터가 {output_path}에 추가 저장되었습니다.\")\n",
    "\n",
    "print(f\"모든 새로 생성된 트리플 데이터가 {output_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
